from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_community.document_loaders import WebBaseLoader
from langchain.schema import Document
import os
import uuid
from typing import List, Dict, Any, Tuple
import asyncio

from dotenv import load_dotenv


load_dotenv()  
class RAGService:
    def __init__(self, vector_store, web_search):
        self.vector_store = vector_store
        self.web_search = web_search
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # LLM ì´ˆê¸°í™”
        if self.openai_api_key:
            self.llm = ChatOpenAI(
                temperature=0.7,
                model="gpt-4o-mini"
            )
        else:
            # OpenAI API í‚¤ê°€ ì—†ì„ ê²½ìš° ëŒ€ì²´ LLM ì‚¬ìš©
            self.llm = self._create_fallback_llm()
        
        # ëŒ€í™” ë©”ëª¨ë¦¬
        self.conversation_memories = {}
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # ëŒ€í™”ë³„ ë²¡í„° ìŠ¤í† ì–´ ìºì‹œ
        self.conversation_vector_stores = {}
    
    def _create_fallback_llm(self):
        """ëŒ€ì²´ LLM ìƒì„± (OpenAI API í‚¤ê°€ ì—†ì„ ê²½ìš°)"""
        class FallbackLLM:
            def __call__(self, prompt: str) -> str:
                # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
                return f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì™„ì „í•œ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸: {prompt}"
        
        return FallbackLLM()
    
    def _get_conversation_collection_name(self, conversation_id: str) -> str:
        """ëŒ€í™”ë³„ ë‹¨ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ì´ë¦„ ìƒì„±"""
        return f"conversation_{conversation_id.replace('-', '_')}"
    
    def _get_long_term_memory_collection_name(self, conversation_id: str) -> str:
        """ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ì´ë¦„ ìƒì„± (ëŒ€í™”ë³„ ë¶„ë¦¬)
        
        ê° ëŒ€í™”ë§ˆë‹¤ ë³„ë„ì˜ ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ì„ ìƒì„±í•˜ì—¬:
        - ëŒ€í™”ë³„ë¡œ ë…ë¦½ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
        - ë‹¤ë¥¸ ëŒ€í™”ì™€ì˜ ì •ë³´ í˜¼ì¬ ë°©ì§€
        - ëŒ€í™”ë³„ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
        """
        return f"long_term_memory_{conversation_id.replace('-', '_')}"
    
    async def _ensure_conversation_collection(self, conversation_id: str) -> Qdrant:
        """ëŒ€í™”ë³„ ë‹¨ê¸°ê¸°ì–µ ì½œë ‰ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        collection_name = self._get_conversation_collection_name(conversation_id)
        
        if collection_name not in self.conversation_vector_stores:
            try:
                # ê¸°ì¡´ ì½œë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
                collections = self.vector_store.client.get_collections()
                collection_exists = any(col.name == collection_name for col in collections.collections)
                
                if collection_exists:
                    print(f"ê¸°ì¡´ ë‹¨ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ì‚¬ìš©: {collection_name}")
                else:
                    # ì½œë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
                    print(f"ìƒˆ ë‹¨ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ìƒì„±: {collection_name}")
                    
                    # OpenAI embeddings ì‚¬ìš© ì‹œ 1536ì°¨ì›, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 384ì°¨ì›
                    vector_size = 1536 if self.openai_api_key else 384
                    
                    self.vector_store.client.create_collection(
                        collection_name,
                        vectors_config={
                            "size": vector_size,
                            "distance": "Cosine"
                        }
                    )
            except Exception as e:
                print(f"ë‹¨ê¸°ê¸°ì–µ ì½œë ‰ì…˜ í™•ì¸/ìƒì„± ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì½œë ‰ì…˜ ì‚¬ìš©
                return self.vector_store
            
            # ëŒ€í™”ë³„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.conversation_vector_stores[collection_name] = Qdrant(
                client=self.vector_store.client,
                collection_name=collection_name,
                embeddings=OpenAIEmbeddings() if self.openai_api_key else None
            )
        
        return self.conversation_vector_stores[collection_name]
    
    async def _ensure_long_term_memory_collection(self, conversation_id: str) -> Qdrant:
        """ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„± (ëŒ€í™”ë³„ ë¶„ë¦¬)"""
        collection_name = self._get_long_term_memory_collection_name(conversation_id)
        
        if collection_name not in self.conversation_vector_stores:
            try:
                # ê¸°ì¡´ ì½œë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
                collections = self.vector_store.client.get_collections()
                collection_exists = any(col.name == collection_name for col in collections.collections)
                
                if collection_exists:
                    print(f"ê¸°ì¡´ ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ì‚¬ìš©: {collection_name}")
                else:
                    # ì½œë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
                    print(f"ìƒˆ ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ ìƒì„±: {collection_name}")
                    
                    # OpenAI embeddings ì‚¬ìš© ì‹œ 1536ì°¨ì›, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 384ì°¨ì›
                    vector_size = 1536 if self.openai_api_key else 384
                    
                    self.vector_store.client.create_collection(
                        collection_name,
                        vectors_config={
                            "size": vector_size,
                            "distance": "Cosine"
                        }
                    )
            except Exception as e:
                print(f"ì¥ê¸°ê¸°ì–µ ì½œë ‰ì…˜ í™•ì¸/ìƒì„± ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì½œë ‰ì…˜ ì‚¬ìš©
                return self.vector_store
            
            # ì¥ê¸°ê¸°ì–µ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.conversation_vector_stores[collection_name] = Qdrant(
                client=self.vector_store.client,
                collection_name=collection_name,
                embeddings=OpenAIEmbeddings() if self.openai_api_key else None
            )
        
        return self.conversation_vector_stores[collection_name]
    
    def _should_use_web_search(self, message: str) -> bool:
        """ë©”ì‹œì§€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        message_lower = message.lower()
        
        # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ í‚¤ì›Œë“œë“¤
        web_search_keywords = [
            'ìµœì‹ ', 'ìµœê·¼', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ì´ë²ˆ ì£¼', 'ì´ë²ˆ ë‹¬', 'ì˜¬í•´',
            'ê²€ìƒ‰', 'ì°¾ì•„', 'ì°¾ê¸°', 'ê²€ìƒ‰í•´', 'ê²€ìƒ‰í•´ì¤˜', 'ê²€ìƒ‰í•´ì£¼ì„¸ìš”',
            'ë‰´ìŠ¤', 'ì†Œì‹', 'ì •ë³´', 'ì—…ë°ì´íŠ¸', 'ë³€ê²½ì‚¬í•­', 'ìƒˆë¡œìš´',
            'ê°€ê²©', 'ì‹œì„¸', 'í™˜ìœ¨', 'ì£¼ì‹', 'ë‚ ì”¨', 'êµí†µ', 'ì§€ë„',
            'ìœ„ì¹˜', 'ì£¼ì†Œ', 'ì „í™”ë²ˆí˜¸', 'ì˜ì—…ì‹œê°„', 'ë¦¬ë·°', 'í‰ì ',
            'ë¹„êµ', 'ì¶”ì²œ', 'ë­í‚¹', 'ìˆœìœ„', 'ì¸ê¸°', 'íŠ¸ë Œë“œ',
            'ì‚¬ì‹¤', 'ì§„ì‹¤', 'í™•ì¸', 'ê²€ì¦', 'ì •í™•í•œ', 'ì •í™•íˆ',
            'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ë¬´ì—‡ì„', 'ì–´ë–»ê²Œ', 'ì™œ',
            'ë„êµ¬', 'tool', 'search', 'find', 'latest', 'current', 'recent',
            'news', 'information', 'update', 'price', 'weather', 'location'
        ]
        
        # í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œ ëª¨ë‘ í™•ì¸
        for keyword in web_search_keywords:
            if keyword in message_lower:
                return True
        
        # íŠ¹ì • ì§ˆë¬¸ íŒ¨í„´ í™•ì¸
        question_patterns = [
            'ë¬´ì—‡', 'ë­', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€',
            'what', 'how', 'why', 'when', 'where', 'who', 'which'
        ]
        
        for pattern in question_patterns:
            if pattern in message_lower:
                return True
        
        # ëª…ë ¹í˜• í‘œí˜„ í™•ì¸
        command_patterns = [
            'ê²€ìƒ‰í•´', 'ì°¾ì•„', 'ì•Œë ¤', 'ë³´ì—¬', 'ê°€ì ¸ì™€', 'ê°€ì ¸ì™€ì¤˜',
            'search', 'find', 'show', 'tell', 'get', 'bring'
        ]
        
        for pattern in command_patterns:
            if pattern in message_lower:
                return True
        
        return False

    async def chat(self, message: str, conversation_id: str = None, use_web_search: bool = True) -> Tuple[str, List[str], str, Dict[str, int]]:
        """ì±—ë´‡ ëŒ€í™” ì²˜ë¦¬ - ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥"""
        try:
            # ë¹ˆ ë©”ì‹œì§€ ì²´í¬
            if not message or not message.strip():
                empty_context_info = {
                    'shortTermMemory': 0,
                    'longTermMemory': 0,
                    'webSearch': 0
                }
                return "ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", [], conversation_id or str(uuid.uuid4()), empty_context_info
            
            # ëŒ€í™” ID ìƒì„±
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if conversation_id not in self.conversation_memories:
                self.conversation_memories[conversation_id] = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True
                )
            
            memory = self.conversation_memories[conversation_id]
            
            # ëŒ€í™”ë³„ ì½œë ‰ì…˜ í™•ì¸/ìƒì„±
            conversation_vector_store = await self._ensure_conversation_collection(conversation_id)
            
            # ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
            should_search = use_web_search and self._should_use_web_search(message)
            
            # 1ë‹¨ê³„: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            sources = []
            if should_search:
                print(f"ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘: {message}")
                search_results = await self.web_search.search(message, max_results=5)
                
                # 2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥
                for result in search_results:
                    if result.get('url'):
                        try:
                            # URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
                            content = await self.web_search.fetch_url_content(result['url'])
                            if content.get('content'):
                                # í…ìŠ¤íŠ¸ ë¶„í• 
                                documents = self.text_splitter.split_text(content['content'])
                                
                                # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ (ëŒ€í™”ë³„ ì½œë ‰ì…˜)
                                doc_objects = []
                                for i, doc_text in enumerate(documents):
                                    doc_objects.append(Document(
                                        page_content=doc_text,
                                        metadata={
                                            'url': result['url'],
                                            'title': content.get('title', ''),
                                            'chunk_index': i,
                                            'total_chunks': len(documents),
                                            'source_url': result['url'],
                                            'search_query': message,
                                            'conversation_id': conversation_id,
                                            'timestamp': asyncio.get_event_loop().time()
                                        }
                                    ))
                                
                                # ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥
                                conversation_vector_store.add_documents(doc_objects)
                                sources.append(result['url'])
                                print(f"URL ì¸ë±ì‹± ì™„ë£Œ: {result['url']} -> {self._get_conversation_collection_name(conversation_id)}")
                        except Exception as e:
                            print(f"URL ì¸ë±ì‹± ì‹¤íŒ¨ {result['url']}: {e}")
                            continue
            else:
                print(f"ì›¹ ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°: {message} (ë¡œì»¬ ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©)")
            
            # 3ë‹¨ê³„: ë‹¨ê¸°ê¸°ì–µ â†’ ì¥ê¸°ê¸°ì–µ â†’ ì›¹ê²€ìƒ‰ ìˆœìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            print(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘: {message}")
            
            # 3-1: ë‹¨ê¸°ê¸°ì–µ (í˜„ì¬ ëŒ€í™”)ì—ì„œ ê²€ìƒ‰
            short_term_context = []
            try:
                print(f"ë‹¨ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹œì‘: {self._get_conversation_collection_name(conversation_id)}")
                short_term_results = conversation_vector_store.similarity_search(message, k=3)
                short_term_context = [result for result in short_term_results if hasattr(result, 'page_content') and result.page_content]
                print(f"ë‹¨ê¸°ê¸°ì–µì—ì„œ {len(short_term_context)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"ë‹¨ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                print(f"ë‹¨ê¸°ê¸°ì–µ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ: {type(conversation_vector_store)}")
                short_term_context = []
            
            # 3-2: ì¥ê¸°ê¸°ì–µ (ëŒ€í™”ë³„ íˆìŠ¤í† ë¦¬)ì—ì„œ ê²€ìƒ‰
            long_term_context = []
            try:
                print(f"ì¥ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹œì‘: {self._get_long_term_memory_collection_name(conversation_id)}")
                long_term_vector_store = await self._ensure_long_term_memory_collection(conversation_id)
                long_term_results = long_term_vector_store.similarity_search(message, k=3)
                long_term_context = [result for result in long_term_results if hasattr(result, 'page_content') and result.page_content]
                print(f"ì¥ê¸°ê¸°ì–µì—ì„œ {len(long_term_context)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"ì¥ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                print(f"ì¥ê¸°ê¸°ì–µ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ: {type(long_term_vector_store) if 'long_term_vector_store' in locals() else 'Not created'}")
                long_term_context = []
            
            # 3-3: ì›¹ê²€ìƒ‰ ê²°ê³¼ë¥¼ í˜„ì¬ ëŒ€í™”ì— ì €ì¥ (ì´ë¯¸ ìˆ˜í–‰ë¨)
            web_search_context = []
            if sources:
                web_search_context = [f"ì›¹ê²€ìƒ‰ ê²°ê³¼: {len(sources)}ê°œ URLì—ì„œ ì •ë³´ ìˆ˜ì§‘ë¨"]
                print(f"ì›¹ê²€ìƒ‰ì—ì„œ {len(sources)}ê°œ ì†ŒìŠ¤ ìˆ˜ì§‘")
            
            # 4ë‹¨ê³„: í†µí•© ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ìš°ì„ ìˆœìœ„: ë‹¨ê¸°ê¸°ì–µ > ì¥ê¸°ê¸°ì–µ > ì›¹ê²€ìƒ‰)
            all_context_docs = []
            
            # ë‹¨ê¸°ê¸°ì–µ ìš°ì„  (ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)
            for result in short_term_context:
                all_context_docs.append(result)
                if hasattr(result, 'metadata') and result.metadata.get('url') and result.metadata.get('url') not in sources:
                    sources.append(result.metadata.get('url'))
            
            # ì¥ê¸°ê¸°ì–µ ì¶”ê°€ (ì¤‘ê°„ ê´€ë ¨ì„±)
            for result in long_term_context:
                all_context_docs.append(result)
                if hasattr(result, 'metadata') and result.metadata.get('url') and result.metadata.get('url') not in sources:
                    sources.append(result.metadata.get('url'))
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._create_context(all_context_docs)
            print(f"í†µí•© ì»¨í…ìŠ¤íŠ¸: ë‹¨ê¸°ê¸°ì–µ {len(short_term_context)}ê°œ, ì¥ê¸°ê¸°ì–µ {len(long_term_context)}ê°œ, ì›¹ê²€ìƒ‰ {len(web_search_context)}ê°œ")
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì •ë³´ ì œê³µ
            if not all_context_docs and not sources:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ AI ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
                context = self._get_default_ai_context(message)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM ì‘ë‹µ
            prompt = self._create_prompt(
                message, 
                context, 
                short_term_count=len(short_term_context),
                long_term_count=len(long_term_context),
                web_search_count=len(sources),
                chat_history=memory.chat_memory.messages # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
            )
            
            print("LLM ì‘ë‹µ ìƒì„± ì¤‘...")
            if hasattr(self.llm, 'invoke'):
                # ìµœì‹  LangChain API
                response = self.llm.invoke(prompt)
                if hasattr(response, 'content'):
                    response = response.content
            else:
                # êµ¬ë²„ì „ í˜¸í™˜ì„±
                response = self.llm(prompt)
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
            memory.chat_memory.add_user_message(message)
            memory.chat_memory.add_ai_message(response)
            
            # 6ë‹¨ê³„: í˜„ì¬ ëŒ€í™”ë¥¼ ì¥ê¸°ê¸°ì–µì— ì €ì¥
            await self._save_to_long_term_memory(conversation_id, message, response, sources)
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
            context_info = {
                'shortTermMemory': len(short_term_context),
                'longTermMemory': len(long_term_context),
                'webSearch': len(sources)
            }
            
            return response, sources, conversation_id, context_info
            
        except Exception as e:
            print(f"ì±—ë´‡ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            error_context_info = {
                'shortTermMemory': 0,
                'longTermMemory': 0,
                'webSearch': 0
            }
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], conversation_id or str(uuid.uuid4()), error_context_info
    
    def _create_context(self, documents: List[Document]) -> str:
        """ë¬¸ì„œë“¤ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        if not documents:
            return ""
        
        context_parts = []
        for doc in documents:
            content = doc.page_content
            title = doc.metadata.get('title', '')
            url = doc.metadata.get('url', '')
            
            if content:
                context_part = f"ì œëª©: {title}\nURL: {url}\në‚´ìš©: {content[:800]}...\n"
                context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _create_prompt(self, message: str, context: str, short_term_count: int = 0, long_term_count: int = 0, web_search_count: int = 0, chat_history: List = None) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„± - ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ë§¥ë½ ì˜ì¡´ì  ì§ˆë¬¸ ì²˜ë¦¬"""
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        chat_history_text = ""
        if chat_history and len(chat_history) > 0:
            chat_history_text = "\n=== ì´ì „ ëŒ€í™” ë‚´ìš© ===\n"
            for i, msg in enumerate(chat_history[-6:], 1):  # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                if hasattr(msg, 'content'):
                    role = "ì‚¬ìš©ì" if hasattr(msg, 'type') and msg.type == 'human' else "AI"
                    chat_history_text += f"{i}. {role}: {msg.content}\n"
            chat_history_text += "==================\n"
        
        if context:
            context_summary = f"""
=== ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ===
- ë‹¨ê¸°ê¸°ì–µ (í˜„ì¬ ëŒ€í™”): {short_term_count}ê°œ ë¬¸ì„œ
- ì¥ê¸°ê¸°ì–µ (ì´ì „ ëŒ€í™” ë‚´ìš©): {long_term_count}ê°œ ë¬¸ì„œ  
- ì›¹ê²€ìƒ‰ ê²°ê³¼: {web_search_count}ê°œ ì†ŒìŠ¤
==================

{context}"""
            
            prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{chat_history_text}

{context_summary}

ì‚¬ìš©ì ì§ˆë¬¸: {message}

ë‹µë³€ ì§€ì¹¨:
1. **ëŒ€í™” ë§¥ë½ íŒŒì•…**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë¨¼ì € í™•ì¸í•˜ì—¬ ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì–¸ê¸‰í–ˆëŠ”ì§€ íŒŒì•…í•˜ì„¸ìš”
2. **ë§¥ë½ ì˜ì¡´ì  ì§ˆë¬¸ ì²˜ë¦¬**: "ê·¸ê±°", "ì´ê²ƒ", "ì €ê²ƒ" ë“±ì˜ ëŒ€ëª…ì‚¬ê°€ ë‚˜ì˜¤ë©´ ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ì°¸ê³ í•˜ì„¸ìš”
3. **ì •ë³´ ìš°ì„ ìˆœìœ„**: ë‹¨ê¸°ê¸°ì–µ(í˜„ì¬ ëŒ€í™”) > ì¥ê¸°ê¸°ì–µ(ì´ì „ ëŒ€í™”) > ì›¹ê²€ìƒ‰ ê²°ê³¼ ìˆœìœ¼ë¡œ ì •ë³´ë¥¼ í™œìš©í•˜ì„¸ìš”
4. **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ì´ì „ ëŒ€í™”ì™€ ì—°ê²°ë˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. **ì •ë³´ ë³´ì™„**: ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ì›¹ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ë³´ì™„í•˜ì„¸ìš”
6. **í•œêµ­ì–´ ë‹µë³€**: í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
7. **ì¶œì²˜ ëª…ì‹œ**: ì°¸ê³ í•œ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”

ë‹µë³€:"""
        else:
            prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{chat_history_text}

ì‚¬ìš©ì ì§ˆë¬¸: {message}

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        return prompt
    
    def _get_default_ai_context(self, message: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ AI ì •ë³´ ì œê³µ"""
        return "ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    def _create_structured_prompt(self, message: str, context: str, chat_history: List = None) -> str:
        """êµ¬ì¡°í™”ëœ ë¶„ì„ ë‹µë³€ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        chat_history_text = ""
        if chat_history and len(chat_history) > 0:
            chat_history_text = "\n=== ì´ì „ ëŒ€í™” ë‚´ìš© ===\n"
            for i, msg in enumerate(chat_history[-6:], 1):  # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                if hasattr(msg, 'content'):
                    role = "ì‚¬ìš©ì" if hasattr(msg, 'type') and msg.type == 'human' else "AI"
                    chat_history_text += f"{i}. {role}: {msg.content}\n"
            chat_history_text += "==================\n"
        
        structured_prompt = f"""ë‹¹ì‹ ì€ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{chat_history_text}

ì‚¬ìš©ì ì§ˆë¬¸: {message}

{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë¶„ì„ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:

## ğŸ“‹ í•µì‹¬ ìš”ì•½
[2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½]

## ğŸ” ì„¸ë¶€ ë¶„ì„
### [ì²« ë²ˆì§¸ ì£¼ì œ/ìš”ì¸]
[êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì˜ˆì‹œ, ë°ì´í„° í¬í•¨]

### [ë‘ ë²ˆì§¸ ì£¼ì œ/ìš”ì¸]
[êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì˜ˆì‹œ, ë°ì´í„° í¬í•¨]

### [ì„¸ ë²ˆì§¸ ì£¼ì œ/ìš”ì¸] (í•„ìš”í•œ ê²½ìš°)
[êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì˜ˆì‹œ, ë°ì´í„° í¬í•¨]

## ğŸ’¡ ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸
[í•µì‹¬ í¬ì¸íŠ¸ì™€ í–¥í›„ ì „ë§, íŠ¸ë Œë“œ ë¶„ì„]

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
- ë¶ˆë¦¿ í¬ì¸íŠ¸(â€¢) í™œìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
- êµ¬ì²´ì ì¸ ë°ì´í„°, í†µê³„, ì˜ˆì‹œ í¬í•¨
- ì‚¬ìš©ì ì¹œí™”ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- ë…¼ë¦¬ì  êµ¬ì¡°ì™€ íë¦„ ìœ ì§€
- í•œêµ­ì–´ë¡œ ë‹µë³€
- ì°¸ê³ í•œ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ê°„ë‹¨íˆ ì–¸ê¸‰

ë‹µë³€:"""
        
        return structured_prompt
    
    async def generate_structured_response(self, message: str, conversation_id: str = None, use_web_search: bool = True) -> Tuple[str, List[str], str, Dict[str, int]]:
        """êµ¬ì¡°í™”ëœ ë¶„ì„ ë‹µë³€ ìƒì„± - ì²´ê³„ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ ì œê³µ"""
        try:
            # ë¹ˆ ë©”ì‹œì§€ ì²´í¬
            if not message or not message.strip():
                empty_context_info = {
                    'shortTermMemory': 0,
                    'longTermMemory': 0,
                    'webSearch': 0
                }
                return "ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", [], conversation_id or str(uuid.uuid4()), empty_context_info
            
            # ëŒ€í™” ID ìƒì„±
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if conversation_id not in self.conversation_memories:
                self.conversation_memories[conversation_id] = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True
                )
            
            memory = self.conversation_memories[conversation_id]
            
            # ëŒ€í™”ë³„ ì½œë ‰ì…˜ í™•ì¸/ìƒì„±
            conversation_vector_store = await self._ensure_conversation_collection(conversation_id)
            
            # ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
            should_search = use_web_search and self._should_use_web_search(message)
            
            # 1ë‹¨ê³„: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            sources = []
            if should_search:
                print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìœ„í•œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘: {message}")
                search_results = await self.web_search.search(message, max_results=8)  # ë” ë§ì€ ì •ë³´ ìˆ˜ì§‘
                
                # 2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥
                for result in search_results:
                    if result.get('url'):
                        try:
                            # URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
                            content = await self.web_search.fetch_url_content(result['url'])
                            if content.get('content'):
                                # í…ìŠ¤íŠ¸ ë¶„í• 
                                documents = self.text_splitter.split_text(content['content'])
                                
                                # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ (ëŒ€í™”ë³„ ì½œë ‰ì…˜)
                                doc_objects = []
                                for i, doc_text in enumerate(documents):
                                    doc_objects.append(Document(
                                        page_content=doc_text,
                                        metadata={
                                            'url': result['url'],
                                            'title': content.get('title', ''),
                                            'chunk_index': i,
                                            'total_chunks': len(documents),
                                            'source_url': result['url'],
                                            'search_query': message,
                                            'conversation_id': conversation_id,
                                            'timestamp': asyncio.get_event_loop().time()
                                        }
                                    ))
                                
                                # ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥
                                conversation_vector_store.add_documents(doc_objects)
                                sources.append(result['url'])
                                print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ìš© URL ì¸ë±ì‹± ì™„ë£Œ: {result['url']}")
                        except Exception as e:
                            print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ìš© URL ì¸ë±ì‹± ì‹¤íŒ¨ {result['url']}: {e}")
                            continue
            else:
                print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìœ„í•œ ì›¹ ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°: {message} (ë¡œì»¬ ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©)")
            
            # 3ë‹¨ê³„: ë‹¨ê¸°ê¸°ì–µ â†’ ì¥ê¸°ê¸°ì–µ â†’ ì›¹ê²€ìƒ‰ ìˆœìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘: {message}")
            
            # 3-1: ë‹¨ê¸°ê¸°ì–µ (í˜„ì¬ ëŒ€í™”)ì—ì„œ ê²€ìƒ‰
            short_term_context = []
            try:
                print(f"ë‹¨ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹œì‘: {self._get_conversation_collection_name(conversation_id)}")
                short_term_results = conversation_vector_store.similarity_search(message, k=5)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                short_term_context = [result for result in short_term_results if hasattr(result, 'page_content') and result.page_content]
                print(f"ë‹¨ê¸°ê¸°ì–µì—ì„œ {len(short_term_context)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"ë‹¨ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                print(f"ë‹¨ê¸°ê¸°ì–µ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ: {type(conversation_vector_store)}")
                short_term_context = []
            
            # 3-2: ì¥ê¸°ê¸°ì–µ (ëŒ€í™”ë³„ íˆìŠ¤í† ë¦¬)ì—ì„œ ê²€ìƒ‰
            long_term_context = []
            try:
                print(f"ì¥ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹œì‘: {self._get_long_term_memory_collection_name(conversation_id)}")
                long_term_vector_store = await self._ensure_long_term_memory_collection(conversation_id)
                long_term_results = long_term_vector_store.similarity_search(message, k=5)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                long_term_context = [result for result in long_term_results if hasattr(result, 'page_content') and result.page_content]
                print(f"ì¥ê¸°ê¸°ì–µì—ì„œ {len(long_term_context)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"ì¥ê¸°ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                print(f"ì¥ê¸°ê¸°ì–µ ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ: {type(long_term_vector_store) if 'long_term_vector_store' in locals() else 'Not created'}")
                long_term_context = []
            
            # 4ë‹¨ê³„: í†µí•© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            all_context_docs = []
            
            # ë‹¨ê¸°ê¸°ì–µ ìš°ì„  (ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)
            for result in short_term_context:
                all_context_docs.append(result)
                if hasattr(result, 'metadata') and result.metadata.get('url') and result.metadata.get('url') not in sources:
                    sources.append(result.metadata.get('url'))
            
            # ì¥ê¸°ê¸°ì–µ ì¶”ê°€ (ì¤‘ê°„ ê´€ë ¨ì„±)
            for result in long_term_context:
                all_context_docs.append(result)
                if hasattr(result, 'metadata') and result.metadata.get('url') and result.metadata.get('url') not in sources:
                    sources.append(result.metadata.get('url'))
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._create_context(all_context_docs)
            print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ìš© í†µí•© ì»¨í…ìŠ¤íŠ¸: ë‹¨ê¸°ê¸°ì–µ {len(short_term_context)}ê°œ, ì¥ê¸°ê¸°ì–µ {len(long_term_context)}ê°œ, ì›¹ê²€ìƒ‰ {len(sources)}ê°œ")
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì •ë³´ ì œê³µ
            if not all_context_docs and not sources:
                print("êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ AI ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
                context = self._get_default_ai_context(message)
            
            # 5ë‹¨ê³„: êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM ì‘ë‹µ
            structured_prompt = self._create_structured_prompt(
                message, 
                context, 
                chat_history=memory.chat_memory.messages
            )
            
            print("êµ¬ì¡°í™”ëœ ë¶„ì„ ë‹µë³€ ìƒì„± ì¤‘...")
            if hasattr(self.llm, 'invoke'):
                # ìµœì‹  LangChain API
                response = self.llm.invoke(structured_prompt)
                if hasattr(response, 'content'):
                    response = response.content
            else:
                # êµ¬ë²„ì „ í˜¸í™˜ì„±
                response = self.llm(structured_prompt)
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
            memory.chat_memory.add_user_message(message)
            memory.chat_memory.add_ai_message(response)
            
            # 6ë‹¨ê³„: í˜„ì¬ ëŒ€í™”ë¥¼ ì¥ê¸°ê¸°ì–µì— ì €ì¥
            await self._save_to_long_term_memory(conversation_id, message, response, sources)
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
            context_info = {
                'shortTermMemory': len(short_term_context),
                'longTermMemory': len(long_term_context),
                'webSearch': len(sources)
            }
            
            return response, sources, conversation_id, context_info
            
        except Exception as e:
            print(f"êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            error_context_info = {
                'shortTermMemory': 0,
                'longTermMemory': 0,
                'webSearch': 0
            }
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], conversation_id or str(uuid.uuid4()), error_context_info
    
    async def generate_topic_based_response(self, message: str, conversation_id: str = None, use_web_search: bool = True) -> Tuple[str, List[str], str, Dict[str, int]]:
        """ì§ˆë¬¸ ë¶„ì„ â†’ ì£¼ì œ ì¶”ì¶œ â†’ ì£¼ì œë³„ ì›¹ ê²€ìƒ‰ â†’ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        try:
            # ë¹ˆ ë©”ì‹œì§€ ì²´í¬
            if not message or not message.strip():
                empty_context_info = {
                    'shortTermMemory': 0,
                    'longTermMemory': 0,
                    'webSearch': 0
                }
                return "ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", [], conversation_id or str(uuid.uuid4()), empty_context_info
            
            # ëŒ€í™” ID ìƒì„±
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if conversation_id not in self.conversation_memories:
                self.conversation_memories[conversation_id] = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True
                )
            
            memory = self.conversation_memories[conversation_id]
            
            print(f"=== ì£¼ì œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œì‘ ===: {message}")
            
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„ ë° ì£¼ì œ ì¶”ì¶œ
            topics = await self._extract_topics_from_question(message)
            print(f"ì¶”ì¶œëœ ì£¼ì œë“¤: {topics}")
            
            # 2ë‹¨ê³„: ì£¼ì œë³„ ì›¹ ê²€ìƒ‰ ë° ì •ë³´ ìˆ˜ì§‘
            topic_research_results = {}
            all_sources = []
            
            if use_web_search and topics:
                for i, topic in enumerate(topics):
                    print(f"ì£¼ì œ {i+1} ê²€ìƒ‰ ì¤‘: {topic}")
                    
                    # ì£¼ì œë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
                    search_keywords = self._generate_search_keywords(topic, message)
                    print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
                    
                    # ì£¼ì œë³„ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                    search_results = await self.web_search.search(search_keywords, max_results=3)
                    
                    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ ë° ì •ë¦¬
                    topic_content = []
                    topic_sources = []
                    
                    for result in search_results:
                        if result.get('url'):
                            try:
                                content = await self.web_search.fetch_url_content(result['url'])
                                if content.get('content'):
                                    # ì£¼ì œì™€ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì¶”ì¶œ
                                    relevant_content = self._extract_relevant_content(content['content'], topic, search_keywords)
                                    if relevant_content:
                                        topic_content.append({
                                            'content': relevant_content,
                                            'url': result['url'],
                                            'title': content.get('title', ''),
                                            'relevance_score': self._calculate_relevance_score(relevant_content, topic)
                                        })
                            except Exception as e:
                                print(f"ì£¼ì œë³„ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ {result['url']}: {e}")
                                continue
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³ , ì •ë ¬ëœ ìˆœì„œì— ë§ì¶° sourcesë„ í•¨ê»˜ ì •ë¦¬
                    topic_content.sort(key=lambda x: x['relevance_score'], reverse=True)
                    
                    # ì •ë ¬ëœ contentì—ì„œ URLì„ ì¶”ì¶œí•˜ì—¬ sources ìƒì„±
                    sorted_sources = [item['url'] for item in topic_content[:2]]  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                    
                    topic_research_results[topic] = {
                        'content': topic_content[:2],  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                        'sources': sorted_sources
                    }
                    
                    # ì „ì²´ ì†ŒìŠ¤ ëª©ë¡ì—ë„ ì¶”ê°€
                    all_sources.extend(sorted_sources)
                    
                    print(f"ì£¼ì œ '{topic}' ê²€ìƒ‰ ì™„ë£Œ: {len(topic_content)}ê°œ ê²°ê³¼, {len(sorted_sources)}ê°œ ì†ŒìŠ¤")
            
            # 3ë‹¨ê³„: êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
            structured_response = await self._generate_topic_based_answer(message, topics, topic_research_results)
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥
            memory.chat_memory.add_user_message(message)
            memory.chat_memory.add_ai_message(structured_response)
            
            # í˜„ì¬ ëŒ€í™”ë¥¼ ì¥ê¸°ê¸°ì–µì— ì €ì¥
            await self._save_to_long_term_memory(conversation_id, message, structured_response, all_sources)
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
            context_info = {
                'shortTermMemory': 0,  # ì£¼ì œ ê¸°ë°˜ ë‹µë³€ì€ ìƒˆë¡œìš´ ê²€ìƒ‰ ê²°ê³¼ì— ì˜ì¡´
                'longTermMemory': 0,
                'webSearch': len(all_sources)
            }
            
            return structured_response, all_sources, conversation_id, context_info
            
        except Exception as e:
            print(f"ì£¼ì œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            error_context_info = {
                'shortTermMemory': 0,
                'longTermMemory': 0,
                'webSearch': 0
            }
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì£¼ì œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", [], conversation_id or str(uuid.uuid4()), error_context_info
    
    async def _extract_topics_from_question(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ ì£¼ì œë“¤ì„ ì¶”ì¶œ"""
        try:
            # ì£¼ì œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            topic_extraction_prompt = f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì£¼ì œë“¤ì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ 3-4ê°œì˜ í•µì‹¬ ì£¼ì œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

1. **ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ íŒŒì•…**: ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì•Œê³  ì‹¶ì–´í•˜ëŠ”ì§€ íŒŒì•…
2. **ì£¼ì œ ë¶„ë¥˜**: ê´€ë ¨ëœ ê°œë…ë“¤ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ê·¸ë£¹í™”
3. **ê²€ìƒ‰ ê°€ëŠ¥í•œ ì£¼ì œ**: ê° ì£¼ì œê°€ ë…ë¦½ì ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ ê°€ëŠ¥í•´ì•¼ í•¨
4. **ì‚¬ëŒë“¤ì´ ê¶ê¸ˆí•´í•  ë§Œí•œ ì£¼ì œ**: ì¼ë°˜ì ìœ¼ë¡œ ê´€ì‹¬ì„ ê°€ì§ˆ ë§Œí•œ ì£¼ì œ

ì¶œë ¥ í˜•ì‹:
- ì£¼ì œ1: [êµ¬ì²´ì ì¸ ì£¼ì œëª…]
    
- ì£¼ì œ2: [êµ¬ì²´ì ì¸ ì£¼ì œëª…]

- ì£¼ì œ3: [êµ¬ì²´ì ì¸ ì£¼ì œëª…]

- ì£¼ì œ4: [êµ¬ì²´ì ì¸ ì£¼ì œëª…] (í•„ìš”í•œ ê²½ìš°)

ì˜ˆì‹œ:
ì§ˆë¬¸: "ë¼ë¶€ë¶€ ë§ì°¨ëŠ” ì™œ ì´ë ‡ê²Œ ë–´ì–´?"
ì£¼ì œë“¤:
- ë¼ë¶€ë¶€ ì•„íŠ¸í† ì´ì˜ ì¸ê¸° ìš”ì¸

- ë§ì°¨ ìŒë£Œ/ë””ì €íŠ¸ì˜ íŠ¸ë Œë“œ

- MZì„¸ëŒ€ì˜ ì†Œë¹„ íŒ¨í„´ ë³€í™”

- SNSì™€ ì…€ëŸ½ì˜ ì˜í–¥

ë‹µë³€:"""

            # LLMì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì œ ì¶”ì¶œ
            if hasattr(self.llm, 'invoke'):
                response = self.llm.invoke(topic_extraction_prompt)
                if hasattr(response, 'content'):
                    response_text = response.content
                else:
                    response_text = str(response)
            else:
                response_text = self.llm(topic_extraction_prompt)
            
            # ì‘ë‹µì—ì„œ ì£¼ì œë“¤ ì¶”ì¶œ
            topics = self._parse_topics_from_response(response_text)
            print(f"LLM ì‘ë‹µ: {response_text}")
            print(f"íŒŒì‹±ëœ ì£¼ì œë“¤: {topics}")
            
            return topics[:4]  # ìµœëŒ€ 4ê°œ ì£¼ì œ
            
        except Exception as e:
            print(f"ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì£¼ì œ ìƒì„±
            return [question]
    
    def _parse_topics_from_response(self, response: str) -> List[str]:
        """LLM ì‘ë‹µì—ì„œ ì£¼ì œë“¤ì„ íŒŒì‹±"""
        topics = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢') or line.startswith('*'):
                # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì œê±°
                topic = line[1:].strip()
                if topic and len(topic) > 3:
                    topics.append(topic)
            elif ':' in line and not line.startswith('ì§ˆë¬¸'):
                # ì½œë¡ ì´ ìˆëŠ” ì¤„ì—ì„œ ì£¼ì œ ì¶”ì¶œ
                parts = line.split(':', 1)
                if len(parts) == 2:
                    topic = parts[1].strip()
                    if topic and len(topic) > 3:
                        topics.append(topic)
        
        # ì£¼ì œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not topics:
            topics = ["ì¼ë°˜ì ì¸ ì •ë³´", "êµ¬ì²´ì ì¸ ì‚¬ë¡€", "ì „ë§ ë° ë¶„ì„"]
        
        return topics
    
    def _generate_search_keywords(self, topic: str, original_question: str) -> str:
        """ì£¼ì œë³„ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        # ì£¼ì œì™€ ì›ë³¸ ì§ˆë¬¸ì„ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        keywords = f"{topic} {original_question}"
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ìµœì í™”
        if any(char in topic for char in 'ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜'):
            # í•œêµ­ì–´ ì£¼ì œì¸ ê²½ìš° ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
            keywords += " í•œêµ­ íŠ¸ë Œë“œ ìµœì‹  ì •ë³´"
        else:
            # ì˜ì–´ ì£¼ì œì¸ ê²½ìš° í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ê°€
            keywords += " í•œêµ­ êµ­ë‚´ í˜„í™© ìµœì‹  ì†Œì‹"
        
        return keywords
    
    def _extract_relevant_content(self, content: str, topic: str, keywords: str) -> str:
        """ì£¼ì œì™€ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ ë‚´ìš© ì¶”ì¶œ
        relevant_sentences = []
        sentences = content.split('.')
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                # ì£¼ì œë‚˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
                if any(keyword.lower() in sentence.lower() for keyword in keywords.split()):
                    relevant_sentences.append(sentence)
        
        if relevant_sentences:
            return '. '.join(relevant_sentences[:3])  # ìƒìœ„ 3ê°œ ë¬¸ì¥ë§Œ ë°˜í™˜
        else:
            # ê´€ë ¨ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì²˜ìŒ 200ì ë°˜í™˜
            return content[:200] + "..." if len(content) > 200 else content
    
    def _calculate_relevance_score(self, content: str, topic: str) -> float:
        """ì½˜í…ì¸ ì™€ ì£¼ì œì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì£¼ì œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì •ë„
        topic_words = topic.lower().split()
        content_lower = content.lower()
        
        for word in topic_words:
            if len(word) > 2:  # 2ê¸€ì ì´í•˜ ë‹¨ì–´ ì œì™¸
                score += content_lower.count(word) * 0.1
        
        # ì½˜í…ì¸  ê¸¸ì´ì— ë”°ë¥¸ ë³´ì •
        if len(content) > 100:
            score += 0.5
        
        return score
    
    async def _generate_topic_based_answer(self, question: str, topics: List[str], research_results: Dict) -> str:
        """ì£¼ì œë³„ ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
        try:
            # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            answer_prompt = f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì œë³„ë¡œ ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì¶”ì¶œëœ ì£¼ì œë“¤:
{chr(10).join([f"- {topic}" for topic in topics])}

ì£¼ì œë³„ ì—°êµ¬ ê²°ê³¼:
{self._format_research_results(research_results)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

## ğŸ“‹ í•µì‹¬ ìš”ì•½
[ì§ˆë¬¸ì˜ í•µì‹¬ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]

## ğŸ” ì£¼ì œë³„ ìƒì„¸ ë¶„ì„

### [ì£¼ì œ1]
- [í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ê³¼ ë¶„ì„]

ì°¸ê³ : [URL1], [URL2]

### [ì£¼ì œ2]
- [í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ê³¼ ë¶„ì„]

ì°¸ê³ : [URL1], [URL2]

### [ì£¼ì œ3]
- [í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ê³¼ ë¶„ì„]

ì°¸ê³ : [URL1], [URL2]

## ğŸ’¡ ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸
[ì „ì²´ì ì¸ ê²°ë¡ ê³¼ í–¥í›„ ì „ë§, ì£¼ëª©í•  ì ]

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
- ê° ì£¼ì œë³„ë¡œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ì°¸ì¡° URLì„ ëª…í™•í•˜ê²Œ í‘œì‹œ
- ì‚¬ìš©ì ì¹œí™”ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- ë…¼ë¦¬ì  êµ¬ì¡°ì™€ íë¦„ ìœ ì§€
- ë¶ˆë¦¿ í¬ì¸íŠ¸(â€¢) í™œìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
- ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì— ë”°ë¥¸ êµ¬ì¡° ì •ë¦¬
- í•œêµ­ì–´ë¡œ ë‹µë³€

ë‹µë³€:"""

            # LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            if hasattr(self.llm, 'invoke'):
                response = self.llm.invoke(answer_prompt)
                if hasattr(response, 'content'):
                    return response.content
                else:
                    return str(response)
            else:
                return self.llm(answer_prompt)
                
        except Exception as e:
            print(f"ì£¼ì œë³„ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì£¼ì œë³„ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _format_research_results(self, research_results: Dict) -> str:
        """ì—°êµ¬ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        formatted = ""
        
        for topic, data in research_results.items():
            formatted += f"\n### {topic}\n"
            
            if data['content']:
                for i, content_item in enumerate(data['content']):
                    formatted += f"ë‚´ìš© {i+1}: {content_item['content'][:300]}...\n"
                    formatted += f"URL {i+1}: {content_item['url']}\n"
            else:
                formatted += "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            
            formatted += f"ì†ŒìŠ¤: {', '.join(data['sources'])}\n"
        
        return formatted
    
    async def _save_to_long_term_memory(self, conversation_id: str, user_message: str, ai_response: str, sources: List[str]) -> None:
        """í˜„ì¬ ëŒ€í™”ë¥¼ ì¥ê¸°ê¸°ì–µì— ì €ì¥ (ëŒ€í™”ë³„ ë¶„ë¦¬)"""
        try:
            long_term_vector_store = await self._ensure_long_term_memory_collection(conversation_id)
            
            # ëŒ€í™” ë‚´ìš©ì„ ë¬¸ì„œë¡œ ë³€í™˜
            conversation_text = f"ì‚¬ìš©ì: {user_message}\n\nAI: {ai_response}"
            if sources:
                conversation_text += f"\n\nì°¸ê³  ì†ŒìŠ¤: {', '.join(sources)}"
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            documents = self.text_splitter.split_text(conversation_text)
            
            # ì¥ê¸°ê¸°ì–µì— ì €ì¥
            doc_objects = []
            for i, doc_text in enumerate(documents):
                doc_objects.append(Document(
                    page_content=doc_text,
                    metadata={
                        'conversation_id': conversation_id,
                        'user_message': user_message,
                        'ai_response': ai_response,
                        'sources': sources,
                        'chunk_index': i,
                        'total_chunks': len(documents),
                        'timestamp': asyncio.get_event_loop().time(),
                        'memory_type': 'long_term'
                    }
                ))
            
            if doc_objects:
                long_term_vector_store.add_documents(doc_objects)
                print(f"ì¥ê¸°ê¸°ì–µì— ì €ì¥ ì™„ë£Œ: ëŒ€í™” {conversation_id} -> {len(doc_objects)}ê°œ ì²­í¬")
            
        except Exception as e:
            print(f"ì¥ê¸°ê¸°ì–µ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def index_urls(self, urls: List[str], conversation_id: str = None) -> int:
        """URLë“¤ì„ ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì¸ë±ì‹±"""
        try:
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
            
            # ëŒ€í™”ë³„ ì½œë ‰ì…˜ í™•ì¸/ìƒì„±
            conversation_vector_store = await self._ensure_conversation_collection(conversation_id)
            
            indexed_count = 0
            
            for url in urls:
                # URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
                content = await self.web_search.fetch_url_content(url)
                
                if content.get('content'):
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    documents = self.text_splitter.split_text(content['content'])
                    
                    # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
                    doc_objects = []
                    for i, doc_text in enumerate(documents):
                        doc_objects.append(Document(
                            page_content=doc_text,
                            metadata={
                                'url': url,
                                'title': content.get('title', ''),
                                'chunk_index': i,
                                'total_chunks': len(documents),
                                'source_url': url,
                                'conversation_id': conversation_id,
                                'timestamp': asyncio.get_event_loop().time()
                            }
                        ))
                    
                    # ëŒ€í™”ë³„ ì½œë ‰ì…˜ì— ì €ì¥
                    conversation_vector_store.add_documents(doc_objects)
                    indexed_count += 1
            
            return indexed_count
            
        except Exception as e:
            print(f"Error indexing URLs: {e}")
            return 0
    
    async def get_conversation_collections(self) -> List[str]:
        """ëª¨ë“  ëŒ€í™” ì½œë ‰ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            collections = await self.vector_store.client.get_collections()
            conversation_collections = []
            
            for collection in collections.collections:
                if collection.name.startswith('conversation_'):
                    conversation_collections.append(collection.name)
            
            return conversation_collections
        except Exception as e:
            print(f"Error getting conversation collections: {e}")
            return []
    
    async def delete_conversation_collection(self, conversation_id: str) -> bool:
        """ëŒ€í™”ë³„ ì½œë ‰ì…˜ ì‚­ì œ"""
        try:
            collection_name = self._get_conversation_collection_name(conversation_id)
            
            # ì½œë ‰ì…˜ ì‚­ì œ
            await self.vector_store.client.delete_collection(collection_name)
            
            # ìºì‹œì—ì„œ ì œê±°
            if collection_name in self.conversation_vector_stores:
                del self.conversation_vector_stores[collection_name]
            
            # ëŒ€í™” ë©”ëª¨ë¦¬ ì œê±°
            if conversation_id in self.conversation_memories:
                del self.conversation_memories[conversation_id]
            
            print(f"ëŒ€í™” ì½œë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {collection_name}")
            return True
            
        except Exception as e:
            print(f"Error deleting conversation collection: {e}")
            return False
    
    def get_conversation_history(self, conversation_id: str) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if conversation_id in self.conversation_memories:
            memory = self.conversation_memories[conversation_id]
            messages = memory.chat_memory.messages
            
            history = []
            for i in range(0, len(messages), 2):
                if i + 1 < len(messages):
                    history.append({
                        'user': messages[i].content,
                        'assistant': messages[i + 1].content
                    })
            
            return history
        
        return []
    
    def clear_conversation(self, conversation_id: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ (ë©”ëª¨ë¦¬ë§Œ)"""
        if conversation_id in self.conversation_memories:
            del self.conversation_memories[conversation_id]
    
    def is_healthy(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            return (
                self.vector_store.is_healthy() and
                self.web_search.is_healthy()
            )
        except Exception:
            return False
