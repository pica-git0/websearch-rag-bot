import requests
from bs4 import BeautifulSoup
import httpx
from typing import List, Dict, Any
import os
import re
from urllib.parse import urljoin, urlparse
import asyncio
import json
import openai

from dotenv import load_dotenv


load_dotenv()  
class WebSearchService:
    def __init__(self):
        self.session = httpx.AsyncClient(timeout=30.0)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Google Custom Search API ì„¤ì •
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")
        self.google_search_url = "https://www.googleapis.com/customsearch/v1"
        
        # DuckDuckGo API (Google APIê°€ ì—†ì„ ë•Œ ëŒ€ì²´)
        self.duckduckgo_url = "https://api.duckduckgo.com/"
        
        # OpenAI API ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
        
        # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë° ëŒ€ì²´ ê²€ìƒ‰ì–´ ë§¤í•‘
        self.query_mappings = {
            'llm': ['LLM', 'large language model', 'AI ëª¨ë¸'],
            'ì‘ë‹µ': ['response', 'answer', 'ë‹µë³€'],
            'ì™„ì„±': ['complete', 'finish', 'ì™„ì„±'],
            'ì‹œì¥': ['market', 'trading', 'ì£¼ì‹'],
            'ì£¼ì‹': ['stock', 'investment', 'íˆ¬ì']
        }
    
    async def search(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ - Google Custom Search API ìš°ì„ , ëŒ€ì²´ë¡œ DuckDuckGo ì‚¬ìš©"""
        try:
            # Google Custom Search API ì‚¬ìš© ì‹œë„
            if self.google_api_key and self.google_cse_id:
                print(f"Google Custom Search API ì‚¬ìš©: {query}")
                search_results = await self._google_search(query, max_results)
                if search_results:
                    return search_results
            
            # Google APIê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° DuckDuckGo ì‚¬ìš©
            print(f"DuckDuckGo API ì‚¬ìš©: {query}")
            search_results = await self._duckduckgo_search(query, max_results)
            if search_results:
                return search_results
            
            # ëª¨ë“  APIê°€ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            print(f"ê¸°ë³¸ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©: {query}")
            return await self._simulate_search(query, max_results)
            
        except Exception as e:
            print(f"Error in web search: {e}")
            return await self._simulate_search(query, max_results)
    
    def _preprocess_query(self, query: str) -> str:
        """ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: í•œêµ­ì–´ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í‚¤ì›Œë“œ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned_query = re.sub(r'[^\w\sê°€-í£]', ' ', query)
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
        english_keywords = []
        for korean_word, english_list in self.query_mappings.items():
            if korean_word in cleaned_query:
                english_keywords.extend(english_list[:2])  # ìµœëŒ€ 2ê°œë§Œ ì¶”ê°€
        
        if english_keywords:
            final_query = f"{cleaned_query} {' '.join(english_keywords)}"
            print(f"ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: '{query}' -> '{final_query}'")
            return final_query
        
        return cleaned_query
    
    async def _create_fallback_query(self, query: str) -> str:
        """ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€ê²½"""
        try:
            # OpenAI APIê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ GPTë¥¼ ì‚¬ìš©
            if self.openai_api_key:
                print(f"GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: {query}")
                
                # GPTì—ê²Œ ê²€ìƒ‰ì–´ë¥¼ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•˜ë„ë¡ ìš”ì²­
                prompt = f"""
                ë‹¤ìŒ ê²€ìƒ‰ì–´ë¥¼ ì›¹ ê²€ìƒ‰ì— ì í•©í•œ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
                ê²€ìƒ‰ì–´: "{query}"
                
                ìš”êµ¬ì‚¬í•­:
                1. ì›ë˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë” ì¼ë°˜ì ì´ê³  ê²€ìƒ‰í•˜ê¸° ì‰¬ìš´ í‚¤ì›Œë“œë¡œ ë³€í™˜
                2. í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨ (í•œêµ­ì–´ ìš°ì„ )
                3. 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì œê³µ
                4. ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ìš©ì–´ ì‚¬ìš©
                5. ì „ë¬¸ ìš©ì–´ëŠ” ì‰¬ìš´ ë™ì˜ì–´ë¡œ ë³€í™˜
                
                ì˜ˆì‹œ:
                - "ê³ ê¸‰ AI ëª¨ë¸" â†’ "AI ëª¨ë¸ ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹"
                - "ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜" â†’ "ì•Œê³ ë¦¬ì¦˜ í”„ë¡œê·¸ë˜ë° ì½”ë”©"
                
                ë³€í™˜ëœ í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
                """
                
                response = openai.ChatCompletion.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ì–´ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ì›¹ ê²€ìƒ‰ì— ì í•©í•œ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=100,
                    temperature=0.3
                )
                
                if response.choices and response.choices[0].message:
                    fallback_query = response.choices[0].message.content.strip()
                    print(f"GPT ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: '{query}' -> '{fallback_query}'")
                    return fallback_query
                else:
                    print("GPT API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
            
        except Exception as e:
            print(f"GPT API ì˜¤ë¥˜: {e}. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
        
        # GPT APIê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
        print("ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
        fallback_parts = []
        for korean_word, english_list in self.query_mappings.items():
            if korean_word in query:
                fallback_parts.append(english_list[0])  # ì²« ë²ˆì§¸ ì˜ì–´ í‚¤ì›Œë“œ ì‚¬ìš©
        
        if fallback_parts:
            fallback_query = ' '.join(fallback_parts)
            print(f"ê¸°ë³¸ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: '{query}' -> '{fallback_query}'")
            return fallback_query
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ë¡œ
        return "AI artificial intelligence"
    
    async def _google_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Google Custom Search APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: í•œêµ­ì–´ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
            processed_query = self._preprocess_query(query)
            
            params = {
                'key': self.google_api_key,
                'cx': self.google_cse_id,
                'q': processed_query,
                'num': min(max_results, 10),  # Google APIëŠ” ìµœëŒ€ 10ê°œ
                'safe': 'active',
                'lr': 'lang_ko',  # í•œêµ­ì–´ ê²°ê³¼ ìš°ì„ 
                'gl': 'kr'  # í•œêµ­ ì§€ì—­ ì„¤ì •
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    self.google_search_url,
                    params=params,
                    timeout=30.0
                )
                response.raise_for_status()
                
                data = response.json()
                
                if 'items' not in data:
                    print(f"Google API ì‘ë‹µì— itemsê°€ ì—†ìŒ: {data}")
                    # ê²€ìƒ‰ì–´ë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë³€ê²½í•´ì„œ ì¬ì‹œë„
                    fallback_query = await self._create_fallback_query(query)
                    if fallback_query != query:
                        print(f"ëŒ€ì²´ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹œë„: {fallback_query}")
                        return await self._google_search(fallback_query, max_results)
                    return []
                
                results = []
                print(f"ğŸ” Google ê²€ìƒ‰ ê²°ê³¼ URLë“¤:")
                for i, item in enumerate(data['items']):
                    result = {
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source': 'google'
                    }
                    results.append(result)
                    
                    # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
                    print(f"  [{i+1}] {result['url']}")
                    print(f"      ì œëª©: {result['title'][:80]}...")
                
                print(f"âœ… Google ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(results)}ê°œ ê²°ê³¼")
                return results[:max_results]
                
        except Exception as e:
            print(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _duckduckgo_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """DuckDuckGo APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        try:
            # DuckDuckGo Instant Answer API
            params = {
                'q': query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    self.duckduckgo_url,
                    params=params,
                    timeout=30.0
                )
                response.raise_for_status()
                
                data = response.json()
                
                results = []
                print(f"ğŸ” DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ URLë“¤:")
                
                # ê´€ë ¨ ì£¼ì œë“¤ ì¶”ê°€
                if 'RelatedTopics' in data:
                    for i, topic in enumerate(data['RelatedTopics'][:max_results]):
                        if 'FirstURL' in topic and 'Text' in topic:
                            result = {
                                'title': topic.get('Text', '')[:100],
                                'url': topic.get('FirstURL', ''),
                                'snippet': topic.get('Text', ''),
                                'source': 'duckduckgo'
                            }
                            results.append(result)
                            
                            # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
                            print(f"  [{len(results)}] {result['url']}")
                            print(f"      ì œëª©: {result['title'][:80]}...")
                
                # ì¶”ìƒ ì •ë³´ ì¶”ê°€
                if 'Abstract' in data and data['Abstract']:
                    result = {
                        'title': data.get('Heading', query),
                        'url': data.get('AbstractURL', ''),
                        'snippet': data.get('Abstract', ''),
                        'source': 'duckduckgo'
                    }
                    results.append(result)
                    
                    # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
                    if result['url']:
                        print(f"  [{len(results)}] {result['url']}")
                        print(f"      ì œëª©: {result['title'][:80]}...")
                
                print(f"âœ… DuckDuckGo ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(results)}ê°œ ê²°ê³¼")
                return results[:max_results]
                
        except Exception as e:
            print(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _simulate_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ëª¨ë“  APIê°€ ì‹¤íŒ¨í•œ ê²½ìš°)"""
        print(f"ğŸ” ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰: {query}")
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI/ê¸°ìˆ  ê´€ë ¨ ì‚¬ì´íŠ¸ë“¤
        sample_results = [
            {
                'title': f'{query} - Wikipedia',
                'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence',
                'snippet': f'{query}ì— ëŒ€í•œ ìœ„í‚¤í”¼ë””ì•„ ì •ë³´ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            },
            {
                'title': f'{query} - OpenAI Blog',
                'url': 'https://openai.com/blog/',
                'snippet': f'{query}ì™€ ê´€ë ¨ëœ OpenAIì˜ ìµœì‹  ì •ë³´ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            },
            {
                'title': f'{query} - Google AI',
                'url': 'https://ai.google/',
                'snippet': f'{query}ì— ëŒ€í•œ Google AIì˜ ì—°êµ¬ ê²°ê³¼ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            }
        ]
        
        # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
        print(f"ğŸ” ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼ URLë“¤:")
        for i, result in enumerate(sample_results[:max_results]):
            print(f"  [{i+1}] {result['url']}")
            print(f"      ì œëª©: {result['title'][:80]}...")
        
        print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(sample_results[:max_results])}ê°œ ê²°ê³¼")
        return sample_results[:max_results]
    
    async def fetch_url_content(self, url: str) -> Dict[str, Any]:
        """URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, headers=self.headers, timeout=30.0)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ''
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: main > article > body)
                main_content = soup.find('main') or soup.find('article') or soup.find('body')
                if main_content:
                    text = main_content.get_text(separator=' ', strip=True)
                else:
                    text = soup.get_text(separator=' ', strip=True)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                text = re.sub(r'\s+', ' ', text).strip()
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ë°©ì§€)
                if len(text) > 10000:
                    text = text[:10000] + "..."
                
                return {
                    'url': url,
                    'title': title_text,
                    'content': text,
                    'metadata': {
                        'charset': response.encoding,
                        'content_type': response.headers.get('content-type', ''),
                        'status_code': response.status_code,
                        'content_length': len(text)
                    }
                }
                
        except Exception as e:
            print(f"Error fetching content from {url}: {e}")
            return {
                'url': url,
                'title': '',
                'content': '',
                'metadata': {'error': str(e)}
            }
    
    async def fetch_multiple_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ URLì—ì„œ ì½˜í…ì¸  ë³‘ë ¬ ì¶”ì¶œ"""
        tasks = [self.fetch_url_content(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì—ëŸ¬ê°€ ë°œìƒí•œ ê²°ê³¼ í•„í„°ë§
        valid_results = []
        for result in results:
            if isinstance(result, dict) and result.get('content'):
                valid_results.append(result)
        
        return valid_results
    
    def extract_links(self, html_content: str, base_url: str) -> List[str]:
        """HTMLì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(base_url, href)
                
                # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                if self._is_valid_url(absolute_url):
                    links.append(absolute_url)
            
            return list(set(links))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and parsed.scheme in ['http', 'https']
        except:
            return False
    
    def is_healthy(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # Google API í‚¤ í™•ì¸
            if self.google_api_key and self.google_cse_id:
                return True
            # DuckDuckGo API í™•ì¸
            return True
        except Exception:
            return False
    
    async def close(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        await self.session.aclose()
