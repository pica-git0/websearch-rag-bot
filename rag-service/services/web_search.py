import requests
from bs4 import BeautifulSoup
import httpx
from typing import List, Dict, Any
import os
import re
from urllib.parse import urljoin, urlparse
import asyncio
import json
import openai

from dotenv import load_dotenv


load_dotenv()  
class WebSearchService:
    def __init__(self):
        self.session = httpx.AsyncClient(timeout=30.0)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Google Custom Search API ì„¤ì •
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")
        self.google_search_url = "https://www.googleapis.com/customsearch/v1"
        

        
        # OpenAI API ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
        
        # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ë° ëŒ€ì²´ ê²€ìƒ‰ì–´ ë§¤í•‘
        self.query_mappings = {
            'llm': ['LLM', 'large language model', 'AI ëª¨ë¸'],
            'ì‘ë‹µ': ['response', 'answer', 'ë‹µë³€'],
            'ì™„ì„±': ['complete', 'finish', 'ì™„ì„±'],
            'ì‹œì¥': ['market', 'trading', 'ì£¼ì‹'],
            'ì£¼ì‹': ['stock', 'investment', 'íˆ¬ì']
        }
    
    async def search(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ - Google Custom Search API ìš°ì„ , ëŒ€ì²´ë¡œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©"""
        try:
            # Google Custom Search API ì‚¬ìš© ì‹œë„
            if self.google_api_key and self.google_cse_id:
                print(f"Google Custom Search API ì‚¬ìš©: {query}")
                search_results = await self._google_search(query, max_results)
                if search_results:
                    return search_results
            
            # Google APIê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
            print(f"ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©: {query}")
            return await self._simulate_search(query, max_results)
            
        except Exception as e:
            print(f"Error in web search: {e}")
            return await self._simulate_search(query, max_results)
    
    def _preprocess_query(self, query: str) -> str:
        """ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: í•œêµ­ì–´ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í‚¤ì›Œë“œ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned_query = re.sub(r'[^\w\sê°€-í£]', ' ', query)
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned_query = re.sub(r'\s+', ' ', cleaned_query).strip()
        
        # ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ê¸¸ë©´ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        if len(cleaned_query) > 50:
            # í•œêµ­ì–´ ë‹¨ì–´ë“¤ì„ ì¶”ì¶œí•˜ê³  ìƒìœ„ 3-4ê°œë§Œ ì‚¬ìš©
            korean_words = re.findall(r'[ê°€-í£]{2,}', cleaned_query)
            if len(korean_words) > 3:
                cleaned_query = ' '.join(korean_words[:3])
                print(f"ê²€ìƒ‰ì–´ ê¸¸ì´ ì œí•œ: '{query}' -> '{cleaned_query}'")
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
        english_keywords = []
        for korean_word, english_list in self.query_mappings.items():
            if korean_word in cleaned_query:
                english_keywords.extend(english_list[:2])  # ìµœëŒ€ 2ê°œë§Œ ì¶”ê°€
        
        if english_keywords:
            final_query = f"{cleaned_query} {' '.join(english_keywords)}"
            print(f"ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: '{query}' -> '{final_query}'")
            return final_query
        
        return cleaned_query
    
    async def _create_fallback_query(self, query: str) -> str:
        """ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€ê²½"""
        try:
            # OpenAI APIê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ GPTë¥¼ ì‚¬ìš©
            if self.openai_api_key:
                print(f"GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: {query}")
                
                # GPTì—ê²Œ ê²€ìƒ‰ì–´ë¥¼ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•˜ë„ë¡ ìš”ì²­
                prompt = f"""
                ë‹¤ìŒ ê²€ìƒ‰ì–´ë¥¼ ì›¹ ê²€ìƒ‰ì— ì í•©í•œ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
                ê²€ìƒ‰ì–´: "{query}"
                
                ìš”êµ¬ì‚¬í•­:
                1. ì›ë˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë” ì¼ë°˜ì ì´ê³  ê²€ìƒ‰í•˜ê¸° ì‰¬ìš´ í‚¤ì›Œë“œë¡œ ë³€í™˜
                2. í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨ (í•œêµ­ì–´ ìš°ì„ )
                3. 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì œê³µ
                4. ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” ì¼ë°˜ì ì¸ ìš©ì–´ ì‚¬ìš©
                5. ì „ë¬¸ ìš©ì–´ëŠ” ì‰¬ìš´ ë™ì˜ì–´ë¡œ ë³€í™˜
                6. ì˜ ëª¨ë¥´ê² ìœ¼ë©´ ê¸°ì¡´ ê²€ìƒ‰ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                
                ì˜ˆì‹œ:
                - "ê³ ê¸‰ AI ëª¨ë¸" â†’ "AI ëª¨ë¸ ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹"
                - "ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜" â†’ "ì•Œê³ ë¦¬ì¦˜ í”„ë¡œê·¸ë˜ë° ì½”ë”©"
                
                ë³€í™˜ëœ í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):
                """
                
                # ìµœì‹  OpenAI API ì‚¬ìš©
                from openai import OpenAI
                client = OpenAI(api_key=self.openai_api_key)
                
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ì–´ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ì›¹ ê²€ìƒ‰ì— ì í•©í•œ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=100,
                    temperature=0.3
                )
                
                if response.choices and response.choices[0].message:
                    fallback_query = response.choices[0].message.content.strip()
                    print(f"GPT ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: '{query}' -> '{fallback_query}'")
                    return fallback_query
                else:
                    print("GPT API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
            
        except Exception as e:
            print(f"GPT API ì˜¤ë¥˜: {e}. ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
        
        # GPT APIê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
        print("ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±")
        fallback_parts = []
        for korean_word, english_list in self.query_mappings.items():
            if korean_word in query:
                fallback_parts.append(english_list[0])  # ì²« ë²ˆì§¸ ì˜ì–´ í‚¤ì›Œë“œ ì‚¬ìš©
        
        if fallback_parts:
            fallback_query = ' '.join(fallback_parts)
            print(f"ê¸°ë³¸ ëŒ€ì²´ ê²€ìƒ‰ì–´ ìƒì„±: '{query}' -> '{fallback_query}'")
            return fallback_query
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ê²€ìƒ‰ì–´ë¡œ
        # ì›ë³¸ ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ ì‹œë„
        core_words = re.findall(r'[ê°€-í£]{2,}', query)
        if core_words:
            # ìƒìœ„ 2ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
            fallback_query = ' '.join(core_words[:2])
            print(f"í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ: '{query}' -> '{fallback_query}'")
            return fallback_query
        
        return "AI artificial intelligence"
    
    async def _google_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Google Custom Search APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬: í•œêµ­ì–´ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
            processed_query = self._preprocess_query(query)
            
            params = {
                'key': self.google_api_key,
                'cx': self.google_cse_id,
                'q': processed_query,
                'num': min(max_results, 10),  # Google APIëŠ” ìµœëŒ€ 10ê°œ
                'safe': 'active',
                'lr': 'lang_ko',  # í•œêµ­ì–´ ê²°ê³¼ ìš°ì„ 
                'gl': 'kr'  # í•œêµ­ ì§€ì—­ ì„¤ì •
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    self.google_search_url,
                    params=params,
                    timeout=30.0
                )
                response.raise_for_status()
                
                data = response.json()
                
                if 'items' not in data:
                    print(f"Google API ì‘ë‹µì— itemsê°€ ì—†ìŒ: {data}")
                    # ê²€ìƒ‰ì–´ë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë³€ê²½í•´ì„œ ì¬ì‹œë„
                    fallback_query = await self._create_fallback_query(query)
                    if fallback_query != query:
                        print(f"ëŒ€ì²´ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹œë„: {fallback_query}")
                        return await self._google_search(fallback_query, max_results)
                    return []
                
                results = []
                print(f"ğŸ” Google ê²€ìƒ‰ ê²°ê³¼ URLë“¤:")
                for i, item in enumerate(data['items']):
                    result = {
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source': 'google'
                    }
                    results.append(result)
                    
                    # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
                    print(f"  [{i+1}] {result['url']}")
                    print(f"      ì œëª©: {result['title'][:80]}...")
                
                print(f"âœ… Google ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(results)}ê°œ ê²°ê³¼")
                return results[:max_results]
                
        except Exception as e:
            print(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    

    
    async def _simulate_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ëª¨ë“  APIê°€ ì‹¤íŒ¨í•œ ê²½ìš°)"""
        print(f"ğŸ” ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰: {query}")
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” AI/ê¸°ìˆ  ê´€ë ¨ ì‚¬ì´íŠ¸ë“¤
        sample_results = [
            {
                'title': f'{query} - Wikipedia',
                'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence',
                'snippet': f'{query}ì— ëŒ€í•œ ìœ„í‚¤í”¼ë””ì•„ ì •ë³´ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            },
            {
                'title': f'{query} - OpenAI Blog',
                'url': 'https://openai.com/blog/',
                'snippet': f'{query}ì™€ ê´€ë ¨ëœ OpenAIì˜ ìµœì‹  ì •ë³´ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            },
            {
                'title': f'{query} - Google AI',
                'url': 'https://ai.google/',
                'snippet': f'{query}ì— ëŒ€í•œ Google AIì˜ ì—°êµ¬ ê²°ê³¼ì…ë‹ˆë‹¤.',
                'source': 'simulation'
            }
        ]
        
        # URLì„ ì½˜ì†”ì— ì¶œë ¥ (ê°œë°œ í™˜ê²½ ëª¨ë‹ˆí„°ë§ìš©)
        print(f"ğŸ” ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼ URLë“¤:")
        for i, result in enumerate(sample_results[:max_results]):
            print(f"  [{i+1}] {result['url']}")
            print(f"      ì œëª©: {result['title'][:80]}...")
        
        print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(sample_results[:max_results])}ê°œ ê²°ê³¼")
        return sample_results[:max_results]
    
    async def fetch_url_content(self, url: str) -> Dict[str, Any]:
        """URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, headers=self.headers, timeout=30.0)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ''
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: main > article > body)
                main_content = soup.find('main') or soup.find('article') or soup.find('body')
                if main_content:
                    text = main_content.get_text(separator=' ', strip=True)
                else:
                    text = soup.get_text(separator=' ', strip=True)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                text = re.sub(r'\s+', ' ', text).strip()
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ë°©ì§€)
                if len(text) > 10000:
                    text = text[:10000] + "..."
                
                return {
                    'url': url,
                    'title': title_text,
                    'content': text,
                    'metadata': {
                        'charset': response.encoding,
                        'content_type': response.headers.get('content-type', ''),
                        'status_code': response.status_code,
                        'content_length': len(text)
                    }
                }
                
        except Exception as e:
            print(f"Error fetching content from {url}: {e}")
            return {
                'url': url,
                'title': '',
                'content': '',
                'metadata': {'error': str(e)}
            }
    
    async def fetch_multiple_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ URLì—ì„œ ì½˜í…ì¸  ë³‘ë ¬ ì¶”ì¶œ"""
        tasks = [self.fetch_url_content(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì—ëŸ¬ê°€ ë°œìƒí•œ ê²°ê³¼ í•„í„°ë§
        valid_results = []
        for result in results:
            if isinstance(result, dict) and result.get('content'):
                valid_results.append(result)
        
        return valid_results
    
    def extract_links(self, html_content: str, base_url: str) -> List[str]:
        """HTMLì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(base_url, href)
                
                # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                if self._is_valid_url(absolute_url):
                    links.append(absolute_url)
            
            return list(set(links))  # ì¤‘ë³µ ì œê±°
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and parsed.scheme in ['http', 'https']
        except:
            return False
    
    def is_healthy(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # Google API í‚¤ í™•ì¸
            if self.google_api_key and self.google_cse_id:
                return True
            # Google APIê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
            return False
        except Exception:
            return False
    
    async def close(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        await self.session.aclose()

    async def classify_search_query(self, query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ì–´ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì ì ˆí•œ ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        try:
            classification_prompt = f"""
ë‹¤ìŒ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ê²€ìƒ‰ì–´: "{query}"

ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬:
1. ì‚¬ëŒ (person): ì¸ë¬¼, ìœ ëª…ì¸, ì „ë¬¸ê°€, ì¼ë°˜ì¸
2. ë™ë¬¼ (animal): ë™ë¬¼, ìƒë¬¼, ë°˜ë ¤ë™ë¬¼
3. ì¶”ìƒì  ê°œë… (concept): ì•„ì´ë””ì–´, ì´ë¡ , ì² í•™, ê°ì •, ìƒíƒœ
4. ê¸°ì—…/ì¡°ì§ (organization): íšŒì‚¬, ë‹¨ì²´, ì •ë¶€ê¸°ê´€, NGO
5. ì‚¬ë¬¼/ì œí’ˆ (object): ë¬¼ê±´, ì œí’ˆ, ë„êµ¬, ì¥ë¹„
6. ì¥ì†Œ (location): ì§€ì—­, êµ­ê°€, ë„ì‹œ, ê±´ë¬¼
7. ì´ë²¤íŠ¸ (event): í–‰ì‚¬, ì¶•ì œ, ê²½ê¸°, íšŒì˜
8. ê¸°íƒ€ (other): ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ê²ƒ

ë¶„ë¥˜ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{{
    "category": "ì¹´í…Œê³ ë¦¬ëª…",
    "confidence": 0.95,
    "subcategory": "ì„¸ë¶€ë¶„ë¥˜",
    "search_strategy": "ê²€ìƒ‰ ì „ëµ",
    "keywords": ["ì¶”ê°€ í‚¤ì›Œë“œ1", "ì¶”ê°€ í‚¤ì›Œë“œ2"]
}}
"""

            # OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜
            if self.openai_api_key:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=self.openai_api_key)
                    
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ì–´ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì¼ê´€ëœ ë¶„ë¥˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                            {"role": "user", "content": classification_prompt}
                        ],
                        max_tokens=200,
                        temperature=0.1
                    )
                    
                    if response.choices and response.choices[0].message:
                        result_text = response.choices[0].message.content.strip()
                        # JSON íŒŒì‹± ì‹œë„
                        try:
                            import json
                            classification = json.loads(result_text)
                            print(f"ğŸ” ê²€ìƒ‰ì–´ ë¶„ë¥˜ ê²°ê³¼: {query} -> {classification['category']} (ì‹ ë¢°ë„: {classification['confidence']})")
                            return classification
                        except json.JSONDecodeError:
                            print(f"JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©: {result_text}")
                
                except Exception as e:
                    print(f"GPT ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë¶„ë¥˜ ì‚¬ìš©")
            
        except Exception as e:
            print(f"ê²€ìƒ‰ì–´ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ ë¶„ë¥˜ (GPT API ì‹¤íŒ¨ ì‹œ)
        return self._basic_query_classification(query)
    
    def _basic_query_classification(self, query: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ê²€ìƒ‰ì–´ ë¶„ë¥˜"""
        query_lower = query.lower()
        
        # ì‚¬ëŒ ë¶„ë¥˜
        person_patterns = [
            r'[ê°€-í£]{2,3}',  # í•œêµ­ì–´ ì´ë¦„ (2-3ê¸€ì)
            r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b',  # ì˜ì–´ ì´ë¦„
            r'ì”¨$|ë‹˜$|êµ°$|ì–‘$',  # í˜¸ì¹­
            r'ê°€ìˆ˜|ë°°ìš°|ì—°ì˜ˆì¸|ì •ì¹˜ì¸|ê¸°ì—…ì¸|í•™ì|ì˜ì‚¬|ë³€í˜¸ì‚¬'
        ]
        
        # ë™ë¬¼ ë¶„ë¥˜
        animal_patterns = [
            r'ê°•ì•„ì§€|ê³ ì–‘ì´|ê°œ|ìƒˆ|ë¬¼ê³ ê¸°|í† ë¼|í–„ìŠ¤í„°|ê±°ë¶ì´|ê³ ë˜|ì‚¬ì|í˜¸ë‘ì´|ì½”ë¼ë¦¬',
            r'dog|cat|bird|fish|rabbit|hamster|turtle|whale|lion|tiger|elephant',
            r'ë™ë¬¼|ìƒë¬¼|ë°˜ë ¤ë™ë¬¼|ì•¼ìƒë™ë¬¼|ì• ì™„ë™ë¬¼'
        ]
        
        # ê¸°ì—…/ì¡°ì§ ë¶„ë¥˜
        organization_patterns = [
            r'íšŒì‚¬|ê¸°ì—…|ê·¸ë£¹|ì£¼ì‹íšŒì‚¬|ãˆœ|ãˆ|corporation|company|inc|corp|ltd',
            r'ì •ë¶€|ì²­|ë¶€|ì²˜|ê¸°ê´€|í˜‘íšŒ|ì¬ë‹¨|ì¬ë‹¨ë²•ì¸|ì‚¬ë‹¨ë²•ì¸',
            r'í•™êµ|ëŒ€í•™êµ|ì´ˆë“±í•™êµ|ì¤‘í•™êµ|ê³ ë“±í•™êµ|university|college|school'
        ]
        
        # ì¥ì†Œ ë¶„ë¥˜
        location_patterns = [
            r'ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì œì£¼|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨',
            r'í•œêµ­|ì¼ë³¸|ì¤‘êµ­|ë¯¸êµ­|ì˜êµ­|í”„ë‘ìŠ¤|ë…ì¼|korea|japan|china|usa|uk|france|germany',
            r'ì‹œ|êµ°|êµ¬|ë™|ì|ë©´|ë„|êµ­|city|country|state|province'
        ]
        
        # ì´ë²¤íŠ¸ ë¶„ë¥˜
        event_patterns = [
            r'ì¶•ì œ|í–‰ì‚¬|ê²½ê¸°|ëŒ€íšŒ|íšŒì˜|ì»¨í¼ëŸ°ìŠ¤|ì„¸ë¯¸ë‚˜|ì›Œí¬ìƒµ|festival|event|game|conference|seminar'
        ]
        
        # ì‚¬ë¬¼/ì œí’ˆ ë¶„ë¥˜
        object_patterns = [
            r'í°|íœ´ëŒ€í°|ìŠ¤ë§ˆíŠ¸í°|ì»´í“¨í„°|ë…¸íŠ¸ë¶|íƒœë¸”ë¦¿|phone|smartphone|computer|laptop|tablet',
            r'ìë™ì°¨|ì°¨|ë²„ìŠ¤|ê¸°ì°¨|ë¹„í–‰ê¸°|car|bus|train|airplane',
            r'ì±…|ì˜í™”|ìŒì•…|ê²Œì„|book|movie|music|game'
        ]
        
        # ì¶”ìƒì  ê°œë… ë¶„ë¥˜
        concept_patterns = [
            r'ì‚¬ë‘|í–‰ë³µ|ìŠ¬í””|ê¸°ì¨|ë¶„ë…¸|ì‚¬ë‘|ìš°ì •|ê°€ì¡±|love|happiness|sadness|joy|anger|friendship|family',
            r'ë¯¼ì£¼ì£¼ì˜|ììœ |í‰ë“±|ì •ì˜|democracy|freedom|equality|justice',
            r'ì˜ˆìˆ |ì² í•™|ê³¼í•™|ê¸°ìˆ |art|philosophy|science|technology'
        ]
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¶„ë¥˜
        if any(re.search(pattern, query_lower) for pattern in person_patterns):
            return {
                "category": "person",
                "confidence": 0.8,
                "subcategory": "ì¸ë¬¼",
                "search_strategy": "ì¸ë¬¼ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["í”„ë¡œí•„", "ê²½ë ¥", "ìˆ˜ìƒ", "í™œë™"]
            }
        elif any(re.search(pattern, query_lower) for pattern in animal_patterns):
            return {
                "category": "animal",
                "confidence": 0.8,
                "subcategory": "ë™ë¬¼",
                "search_strategy": "ë™ë¬¼ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["íŠ¹ì§•", "ìŠµì„±", "ì‚¬ìœ¡ë²•", "ì •ë³´"]
            }
        elif any(re.search(pattern, query_lower) for pattern in organization_patterns):
            return {
                "category": "organization",
                "confidence": 0.8,
                "subcategory": "ê¸°ì—…/ì¡°ì§",
                "search_strategy": "ê¸°ì—… ì •ë³´ ê²€ìƒ‰",
                "keywords": ["íšŒì‚¬ ì •ë³´", "ì‚¬ì—…", "ì—°í˜", "ë‰´ìŠ¤"]
            }
        elif any(re.search(pattern, query_lower) for pattern in location_patterns):
            return {
                "category": "location",
                "confidence": 0.8,
                "subcategory": "ì¥ì†Œ",
                "search_strategy": "ì§€ì—­ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["ê´€ê´‘", "ì—­ì‚¬", "ë¬¸í™”", "ì •ë³´"]
            }
        elif any(re.search(pattern, query_lower) for pattern in event_patterns):
            return {
                "category": "event",
                "confidence": 0.8,
                "subcategory": "ì´ë²¤íŠ¸",
                "search_strategy": "ì´ë²¤íŠ¸ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["ì¼ì •", "ì¥ì†Œ", "ì°¸ê°€", "ì •ë³´"]
            }
        elif any(re.search(pattern, query_lower) for pattern in object_patterns):
            return {
                "category": "object",
                "confidence": 0.8,
                "subcategory": "ì‚¬ë¬¼/ì œí’ˆ",
                "search_strategy": "ì œí’ˆ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["ìŠ¤í™", "ê°€ê²©", "ë¦¬ë·°", "êµ¬ë§¤"]
            }
        elif any(re.search(pattern, query_lower) for pattern in concept_patterns):
            return {
                "category": "concept",
                "confidence": 0.8,
                "subcategory": "ì¶”ìƒì  ê°œë…",
                "search_strategy": "ê°œë… ì •ë³´ ê²€ìƒ‰",
                "keywords": ["ì •ì˜", "ì˜ˆì‹œ", "ê´€ë ¨", "ì •ë³´"]
            }
        else:
            return {
                "category": "other",
                "confidence": 0.5,
                "subcategory": "ê¸°íƒ€",
                "search_strategy": "ì¼ë°˜ ì •ë³´ ê²€ìƒ‰",
                "keywords": ["ì •ë³´", "ë‰´ìŠ¤", "ìµœì‹ ", "íŠ¸ë Œë“œ"]
            }
